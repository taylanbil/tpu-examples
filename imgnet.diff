19,27d18
<     '--lr_scheduler_type': {
<         'type': str,
<     },
<     '--lr_scheduler_divide_every_n_epochs': {
<         'type': int,
<     },
<     '--lr_scheduler_divisor': {
<         'type': int,
<     },
40d30
< from common_utils import TestCase, run_tests
42d31
< import schedulers
56c45
< import unittest
---
> import torch_xla_py.xla_multiprocessing as xmp
66,72c55
< MODEL_SPECIFIC_DEFAULTS = {
<     'resnet50': dict({
<         'lr_scheduler_divide_every_n_epochs': 20,
<         'lr_scheduler_divisor': 5,
<         'lr_scheduler_type': 'WarmupAndExponentialDecayScheduler',
<       }, **DEFAULT_KWARGS)
< }
---
> MODEL_SPECIFIC_DEFAULTS = {}
132d114
<     test_sampler = None
135,139c117,120
<           train_dataset, num_replicas=xm.xrt_world_size(),
<           rank=xm.get_ordinal(), shuffle=True)
<       test_sampler = torch.utils.data.distributed.DistributedSampler(
<           test_dataset, num_replicas=xm.xrt_world_size(),
<           rank=xm.get_ordinal(), shuffle=False)
---
>           train_dataset,
>           num_replicas=xm.xrt_world_size(),
>           rank=xm.get_ordinal(),
>           shuffle=True)
149d129
<         sampler=test_sampler,
155,179c135,144
<   devices = (
<       xm.get_xla_supported_devices(
<           max_devices=FLAGS.num_cores) if FLAGS.num_cores != 0 else [])
<   # Pass [] as device_ids to run using the PyTorch/CPU engine.
<   torchvision_model = get_model_property('model_fn')
<   model_parallel = dp.DataParallel(torchvision_model, device_ids=devices)
< 
<   def train_loop_fn(model, loader, device, context):
<     loss_fn = nn.CrossEntropyLoss()
<     optimizer = context.getattr_or(
<         'optimizer', lambda: optim.SGD(
<             model.parameters(),
<             lr=FLAGS.lr,
<             momentum=FLAGS.momentum,
<             weight_decay=5e-4))
<     lr_scheduler = context.getattr_or(
<         'lr_scheduler', lambda: schedulers.wrap_optimizer_with_scheduler(
<             optimizer,
<             scheduler_type=getattr(FLAGS, 'lr_scheduler_type', None),
<             scheduler_divisor=getattr(FLAGS, 'lr_scheduler_divisor', None),
<             scheduler_divide_every_n_epochs=getattr(
<                 FLAGS, 'lr_scheduler_divide_every_n_epochs', None),
<             num_steps_per_epoch=num_training_steps_per_epoch,
<             summary_writer=writer if test_utils.is_first_device(
<                 device, devices) else None))
---
>   device = xm.xla_device()
>   model = get_model_property('model_fn')().to(device)
>   optimizer = optim.SGD(
>       model.parameters(),
>       lr=FLAGS.lr,
>       momentum=FLAGS.momentum,
>       weight_decay=5e-4)
>   loss_fn = nn.CrossEntropyLoss()
> 
>   def train_loop_fn(loader):
190,191c155
<         test_utils.print_training_update(device, x, loss.item(),
<                                          tracker.rate(),
---
>         test_utils.print_training_update(device, x, loss.item(), tracker.rate(),
193,194d156
<       if lr_scheduler:
<         lr_scheduler.step()
196c158
<   def test_loop_fn(model, loader, device, context):
---
>   def test_loop_fn(loader):
212,215d173
<   num_devices = len(
<       xm.xla_replication_devices(devices)) if len(devices) > 1 else 1
<   num_training_steps_per_epoch = len(train_dataset.imgs) // (
<       FLAGS.batch_size * num_devices)
217,223c175,182
<     model_parallel(train_loop_fn, train_loader)
<     accuracies = model_parallel(test_loop_fn, test_loader)
<     accuracy = mean(accuracies)
<     print("Epoch: {}, Mean Accuracy: {:.2f}%".format(epoch, accuracy))
<     global_step = (epoch - 1) * num_training_steps_per_epoch
<     test_utils.add_scalar_to_summary(writer, 'Accuracy/test', accuracy,
<                                      global_step)
---
>     para_loader = dp.ParallelLoader(train_loader, [device])
>     train_loop_fn(para_loader.per_device_loader(device))
> 
>     para_loader = dp.ParallelLoader(test_loader, [device])
>     accuracy = test_loop_fn(para_loader.per_device_loader(device))
>     print('Epoch: {}, Mean Accuracy: {:.2f}%'.format(epoch, accuracy))
>     test_utils.add_scalar_to_summary(writer, 'Accuracy/test', accuracy, epoch)
> 
230,236c189,197
< class TrainImageNet(TestCase):
< 
<   def tearDown(self):
<     super(TrainImageNet, self).tearDown()
< 
<   def test_accurracy(self):
<     self.assertGreaterEqual(train_imagenet(), FLAGS.target_accuracy)
---
> def _mp_fn(index, flags):
>   global FLAGS
>   FLAGS = flags
>   torch.set_default_tensor_type('torch.FloatTensor')
>   accuracy = train_imagenet()
>   if accuracy < FLAGS.target_accuracy:
>     print('Accuracy {} is below target {}'.format(accuracy,
>                                                   FLAGS.target_accuracy))
>     sys.exit(21)
239,241c200,201
< # Run the tests.
< torch.set_default_tensor_type('torch.FloatTensor')
< run_tests()
---
> if __name__ == '__main__':
>   xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS.num_cores)
