31d30
< from common_utils import TestCase, run_tests
45c44
< import unittest
---
> import torch_xla_py.xla_multiprocessing as xmp
115d113
<     test_sampler = None
118,122c116,119
<           train_dataset, num_replicas=xm.xrt_world_size(),
<           rank=xm.get_ordinal(), shuffle=True)
<       test_sampler = torch.utils.data.distributed.DistributedSampler(
<           test_dataset, num_replicas=xm.xrt_world_size(),
<           rank=xm.get_ordinal(), shuffle=False)
---
>           train_dataset,
>           num_replicas=xm.xrt_world_size(),
>           rank=xm.get_ordinal(),
>           shuffle=True)
132d128
<         sampler=test_sampler,
138,152c134,143
<   devices = (
<       xm.get_xla_supported_devices(
<           max_devices=FLAGS.num_cores) if FLAGS.num_cores != 0 else [])
<   # Pass [] as device_ids to run using the PyTorch/CPU engine.
<   torchvision_model = get_model_property('model_fn')
<   model_parallel = dp.DataParallel(torchvision_model, device_ids=devices)
< 
<   def train_loop_fn(model, loader, device, context):
<     loss_fn = nn.CrossEntropyLoss()
<     optimizer = context.getattr_or(
<         'optimizer', lambda: optim.SGD(
<             model.parameters(),
<             lr=FLAGS.lr,
<             momentum=FLAGS.momentum,
<             weight_decay=5e-4))
---
>   device = xm.xla_device()
>   model = get_model_property('model_fn')().to(device)
>   optimizer = optim.SGD(
>       model.parameters(),
>       lr=FLAGS.lr,
>       momentum=FLAGS.momentum,
>       weight_decay=5e-4)
>   loss_fn = nn.CrossEntropyLoss()
> 
>   def train_loop_fn(loader):
163,164c154
<         test_utils.print_training_update(device, x, loss.item(),
<                                          tracker.rate(),
---
>         test_utils.print_training_update(device, x, loss.item(), tracker.rate(),
167c157
<   def test_loop_fn(model, loader, device, context):
---
>   def test_loop_fn(loader):
184,187c174,179
<     model_parallel(train_loop_fn, train_loader)
<     accuracies = model_parallel(test_loop_fn, test_loader)
<     accuracy = mean(accuracies)
<     print("Epoch: {}, Mean Accuracy: {:.2f}%".format(epoch, accuracy))
---
>     para_loader = dp.ParallelLoader(train_loader, [device])
>     train_loop_fn(para_loader.per_device_loader(device))
> 
>     para_loader = dp.ParallelLoader(test_loader, [device])
>     accuracy = test_loop_fn(para_loader.per_device_loader(device))
>     print('Epoch: {}, Mean Accuracy: {:.2f}%'.format(epoch, accuracy))
188a181
> 
195,201c188,196
< class TrainImageNet(TestCase):
< 
<   def tearDown(self):
<     super(TrainImageNet, self).tearDown()
< 
<   def test_accurracy(self):
<     self.assertGreaterEqual(train_imagenet(), FLAGS.target_accuracy)
---
> def _mp_fn(index, flags):
>   global FLAGS
>   FLAGS = flags
>   torch.set_default_tensor_type('torch.FloatTensor')
>   accuracy = train_imagenet()
>   if accuracy < FLAGS.target_accuracy:
>     print('Accuracy {} is below target {}'.format(accuracy,
>                                                   FLAGS.target_accuracy))
>     sys.exit(21)
204,206c199,200
< # Run the tests.
< torch.set_default_tensor_type('torch.FloatTensor')
< run_tests()
---
> if __name__ == '__main__':
>   xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS.num_cores)
